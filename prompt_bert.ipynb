{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "_I7O9YBZnYsq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "505\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>557138</th>\n",
              "      <td>0</td>\n",
              "      <td>wants to compete! i want hard competition! i w...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>349381</th>\n",
              "      <td>0</td>\n",
              "      <td>It seems we are stuck on the ground in Amarill...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>182051</th>\n",
              "      <td>0</td>\n",
              "      <td>where the f are my pinking shears? rarararrrar...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>571236</th>\n",
              "      <td>0</td>\n",
              "      <td>0ff t0 tHE MEEtiN..  i HAtE WhEN PPl V0lUNtEER...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1339637</th>\n",
              "      <td>4</td>\n",
              "      <td>@ reply me pls</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>235274</th>\n",
              "      <td>0</td>\n",
              "      <td>Sitting with a temper bed cover over my bed Re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>122382</th>\n",
              "      <td>0</td>\n",
              "      <td>ouch just got a nasty burn</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>419988</th>\n",
              "      <td>0</td>\n",
              "      <td>Lost my headphones tonight.. Lame   1 week til...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>241280</th>\n",
              "      <td>0</td>\n",
              "      <td>THE HILLS SEASON FNALE...tonight</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>661292</th>\n",
              "      <td>0</td>\n",
              "      <td>@jcbaggee No &amp;quot;FUCKING&amp;quot; here, tried s...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1000 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         label                                           sentence\n",
              "557138       0  wants to compete! i want hard competition! i w...\n",
              "349381       0  It seems we are stuck on the ground in Amarill...\n",
              "182051       0  where the f are my pinking shears? rarararrrar...\n",
              "571236       0  0ff t0 tHE MEEtiN..  i HAtE WhEN PPl V0lUNtEER...\n",
              "1339637      4                                    @ reply me pls \n",
              "...        ...                                                ...\n",
              "235274       0  Sitting with a temper bed cover over my bed Re...\n",
              "122382       0                        ouch just got a nasty burn \n",
              "419988       0  Lost my headphones tonight.. Lame   1 week til...\n",
              "241280       0                  THE HILLS SEASON FNALE...tonight \n",
              "661292       0  @jcbaggee No &quot;FUCKING&quot; here, tried s...\n",
              "\n",
              "[1000 rows x 2 columns]"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('data/training.1600000.processed.noemoticon.csv', header=None, encoding='latin-1')\n",
        "df = df.loc[:, [0, 5]]\n",
        "df.columns = ['label', 'sentence']\n",
        "df=df.sample(frac=1, random_state=0)[:1000]\n",
        "print(df[df['label']==0].shape[0])\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "2dTAy4c5pcbt"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>sentence</th>\n",
              "      <th>token</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>557138</th>\n",
              "      <td>0</td>\n",
              "      <td>wants to compete! i want hard competition! i w...</td>\n",
              "      <td>[wants, to, compete, i, want, hard, competitio...</td>\n",
              "      <td>wants to compete i want hard competition i wan...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>349381</th>\n",
              "      <td>0</td>\n",
              "      <td>It seems we are stuck on the ground in Amarill...</td>\n",
              "      <td>[it, seems, we, are, stuck, on, the, ground, i...</td>\n",
              "      <td>it seems we are stuck on the ground in amarill...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>182051</th>\n",
              "      <td>0</td>\n",
              "      <td>where the f are my pinking shears? rarararrrar...</td>\n",
              "      <td>[where, the, f, are, my, pinking, shears, rara...</td>\n",
              "      <td>where the f are my pinking shears rarararrrara...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>571236</th>\n",
              "      <td>0</td>\n",
              "      <td>0ff t0 tHE MEEtiN..  i HAtE WhEN PPl V0lUNtEER...</td>\n",
              "      <td>[0ff, t0, the, meetin, i, hate, when, ppl, v0l...</td>\n",
              "      <td>0ff t0 the meetin i hate when ppl v0lunteer my...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1339637</th>\n",
              "      <td>4</td>\n",
              "      <td>@ reply me pls</td>\n",
              "      <td>[reply, me, pls]</td>\n",
              "      <td>reply me pls</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         label                                           sentence  \\\n",
              "557138       0  wants to compete! i want hard competition! i w...   \n",
              "349381       0  It seems we are stuck on the ground in Amarill...   \n",
              "182051       0  where the f are my pinking shears? rarararrrar...   \n",
              "571236       0  0ff t0 tHE MEEtiN..  i HAtE WhEN PPl V0lUNtEER...   \n",
              "1339637      4                                    @ reply me pls    \n",
              "\n",
              "                                                     token  \\\n",
              "557138   [wants, to, compete, i, want, hard, competitio...   \n",
              "349381   [it, seems, we, are, stuck, on, the, ground, i...   \n",
              "182051   [where, the, f, are, my, pinking, shears, rara...   \n",
              "571236   [0ff, t0, the, meetin, i, hate, when, ppl, v0l...   \n",
              "1339637                                   [reply, me, pls]   \n",
              "\n",
              "                                                      text  \n",
              "557138   wants to compete i want hard competition i wan...  \n",
              "349381   it seems we are stuck on the ground in amarill...  \n",
              "182051   where the f are my pinking shears rarararrrara...  \n",
              "571236   0ff t0 the meetin i hate when ppl v0lunteer my...  \n",
              "1339637                                       reply me pls  "
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "from string import punctuation\n",
        "def sentence_clean(x):\n",
        "  x = x.lower() # lower capitals\n",
        "  x = re.sub(r'@[A-Za-z0-9\\._]*', '', x) # remove @XXX\n",
        "  x = re.sub(r'[A-Za-z]+://[^\\s]*', '', x) # remove XXX://XXX\n",
        "  x = re.sub(r'[{}]+'.format(punctuation), '', x) # remove punctuation\n",
        "  x = re.sub(r':\\)', 'smile', x) # transfer :) to smile\n",
        "  x = re.sub(r':\\(', 'sad', x) # transfer :( to sad\n",
        "  x = re.sub(r' +', ' ', x)\n",
        "  return x.split() # token\n",
        "\n",
        "df['token'] = df['sentence'].apply(sentence_clean)\n",
        "df['text'] = df['token'].apply(lambda t: ' '.join(t))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "jaTegglAskq-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AdamW, BertConfig, BertTokenizer, BertForMaskedLM\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import torch.nn as nn \n",
        "\n",
        "def seed(seed=1):\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.random.manual_seed(seed)\n",
        "  np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "LNmhdWf2xmwp"
      },
      "outputs": [],
      "source": [
        "full_text = df.text.values\n",
        "full_label = df.label.values\n",
        "\n",
        "for i in range(full_text.shape[0]):\n",
        "  full_text[i] += \" [SEP] this sentence sentiment is [MASK]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "APjFeu16xd7R"
      },
      "outputs": [],
      "source": [
        "div = int(0.1 * full_text.shape[0])\n",
        "p = np.random.permutation(full_text.shape[0])\n",
        "shuffled_full_text, shuffled_full_label = full_text[p], full_label[p]\n",
        "train_text, train_label = shuffled_full_text[div: ], shuffled_full_label[div: ]\n",
        "dev_text, dev_label = shuffled_full_text[: div], shuffled_full_label[: div]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "WiczE8sEzhY9"
      },
      "outputs": [],
      "source": [
        "train_labels = []\n",
        "dev_labels = []\n",
        "label_dict = {0: 0, 4: 1}\n",
        "for label in train_label:\n",
        "  train_labels.append(label_dict[label])\n",
        "\n",
        "for label in dev_label:\n",
        "  dev_labels.append(label_dict[label])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "7T6_2VWz8W_z"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "74-GM7PT0JNk"
      },
      "outputs": [],
      "source": [
        "MODEL = 'bert-large-uncased-whole-word-masking'\n",
        "\n",
        "class PModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.numClass = 2\n",
        "\n",
        "    self.MaskModel = BertForMaskedLM.from_pretrained(MODEL).bert\n",
        "    self.MaskModel.config.output_hidden_states=True\n",
        "    self.tokenizer = BertTokenizer.from_pretrained(MODEL)\n",
        "    self.lastLayer = nn.Sequential(nn.Linear(self.MaskModel.config.hidden_size, self.numClass), nn.Tanh())\n",
        "    self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "  def forward(self, input_ids, attention_masks, token_type_ids):\n",
        "    mask_indices = torch.where(self.tokenizer.mask_token_id == input_ids)\n",
        "    last_hidden_states = self.MaskModel(input_ids=input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids).hidden_states[-1][mask_indices] # (bz, hidden_size) if only one mask\n",
        "    y = self.lastLayer(last_hidden_states)\n",
        "    y = self.softmax(y)\n",
        "    return y\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers.models.bert.tokenization_bert import BertTokenizer\n",
        "from transformers.models.bert.tokenization_bert_fast import BertTokenizerFast\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer:BertTokenizerFast = AutoTokenizer.from_pretrained(MODEL, use_fast=True)\n",
        "def processdata(texts, labels):\n",
        "  encoded_dict = tokenizer(list(texts),\n",
        "                      max_length = 128,\n",
        "                      pad_to_max_length = True,\n",
        "                      return_attention_mask = True,\n",
        "                      padding= 'max_length',\n",
        "                      truncation= True,\n",
        "                      return_tensors = 'pt',\n",
        "                      return_token_type_ids = True,\n",
        "                  )\n",
        "  input_ids = encoded_dict['input_ids']\n",
        "  attention_masks = encoded_dict['attention_mask']\n",
        "  token_type_ids = encoded_dict['token_type_ids']\n",
        "  labels = torch.tensor(labels)\n",
        "  return input_ids, attention_masks, token_type_ids, labels\n",
        "\n",
        "train_input_ids, train_attention_masks, train_token_type_ids, train_labels = processdata(train_text, train_labels)\n",
        "dev_input_ids, dev_attention_masks, dev_token_type_ids, dev_labels = processdata(dev_text, dev_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "tE-WqTkcAioB"
      },
      "outputs": [],
      "source": [
        "batch_size = 16\n",
        "\n",
        "train_dataset = TensorDataset(train_input_ids, train_attention_masks, train_token_type_ids, train_labels, torch.arange(train_labels.size(0)))\n",
        "dev_dataset = TensorDataset(dev_input_ids, dev_attention_masks, dev_token_type_ids, dev_labels, torch.arange(dev_labels.size(0)))\n",
        "\n",
        "train_loader = DataLoader(\n",
        "            train_dataset,\n",
        "            sampler = RandomSampler(train_dataset),\n",
        "            batch_size = batch_size \n",
        "        )\n",
        "\n",
        "\n",
        "dev_loader = DataLoader(\n",
        "            dev_dataset, \n",
        "            sampler = SequentialSampler(dev_dataset), \n",
        "            batch_size = batch_size\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([900, 128])\n",
            "torch.Size([900, 128])\n",
            "torch.Size([900, 128])\n",
            "torch.Size([900])\n",
            "57\n"
          ]
        }
      ],
      "source": [
        "print(train_input_ids.shape)\n",
        "print(train_attention_masks.shape)\n",
        "print(train_token_type_ids.shape)\n",
        "print(train_labels.shape)\n",
        "\n",
        "print(len(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "rHvnT29pwocx"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "/conda_envs/pynli/lib/python3.7/site-packages/transformers/optimization.py:310: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  FutureWarning,\n"
          ]
        }
      ],
      "source": [
        "model = PModel()\n",
        "model.to(device)\n",
        "model.cuda()\n",
        "\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "          )\n",
        "EPOCHS = 10\n",
        "total_steps = len(train_loader) * EPOCHS\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                          num_warmup_steps = 100,\n",
        "                          num_training_steps = total_steps)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "6DXpzW6BD-Pp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([900, 128])\n",
            "total step:  57\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "57it [00:12,  4.67it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss:  0.6570307361452203\n",
            "dev loss:  0.5654321823801313\n",
            "saving checkpoint\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "57it [00:12,  4.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss:  0.5438366119276014\n",
            "dev loss:  0.5427388335977282\n",
            "saving checkpoint\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "57it [00:12,  4.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss:  0.4971362796791813\n",
            "dev loss:  0.5403973119599479\n",
            "saving checkpoint\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "57it [00:12,  4.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss:  0.4768465459346771\n",
            "dev loss:  0.5469534184251513\n",
            "skip saving checkpoint\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "57it [00:12,  4.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss:  0.4621815634401221\n",
            "dev loss:  0.5369722587721688\n",
            "saving checkpoint\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "57it [00:12,  4.63it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss:  0.4506804320895881\n",
            "dev loss:  0.5498600474425724\n",
            "skip saving checkpoint\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "57it [00:12,  4.62it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss:  0.44493296533300164\n",
            "dev loss:  0.5514935382774898\n",
            "skip saving checkpoint\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "57it [00:12,  4.60it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss:  0.43085008755064846\n",
            "dev loss:  0.5323380700179509\n",
            "saving checkpoint\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "57it [00:12,  4.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss:  0.4292616561839455\n",
            "dev loss:  0.5175131218773978\n",
            "saving checkpoint\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "57it [00:12,  4.61it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "train loss:  0.4258292542214979\n",
            "dev loss:  0.536018852676664\n",
            "skip saving checkpoint\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "total_step = len(train_loader)\n",
        "print(train_input_ids.shape)\n",
        "print('total step: ', total_step)\n",
        "criteria = nn.CrossEntropyLoss()\n",
        "\n",
        "best_dev_loss = 100\n",
        "for epoch in range(EPOCHS):\n",
        "  model.train()\n",
        "\n",
        "  total_train_loss = 0\n",
        "  total_train_acc  = 0\n",
        "  for batch_idx, (pair_token_ids, mask_ids, seg_ids, y, _) in tqdm(enumerate(train_loader)):\n",
        "    pair_token_ids = pair_token_ids.to(device)\n",
        "    mask_ids = mask_ids.to(device)\n",
        "    seg_ids = seg_ids.to(device)\n",
        "    labels = y.to(device)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    prediction = model(pair_token_ids, mask_ids, seg_ids)\n",
        "    try:\n",
        "      loss = criteria(prediction, labels)\n",
        "    except:\n",
        "      print('exception: too long a sentence, skipping the batch')\n",
        "      continue\n",
        "    \n",
        "    total_train_loss += loss.item()\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    # Clip the norm of the gradients to 1.0.\n",
        "    # This is to help prevent the \"exploding gradients\" problem.\n",
        "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "    optimizer.step()\n",
        "    scheduler.step()\n",
        "\n",
        "\n",
        "  train_loss = total_train_loss/len(train_loader)\n",
        "  print('train loss: ', train_loss)\n",
        "\n",
        "  # Put the model in evaluation mode\n",
        "  model.eval()\n",
        "\n",
        "  total_dev_loss = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for batch_idx, (pair_token_ids, mask_ids, seg_ids, y, _) in enumerate(dev_loader):\n",
        "\n",
        "      #clear any previously calculated gradients before performing a backward pass\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      pair_token_ids = pair_token_ids.to(device)\n",
        "      mask_ids = mask_ids.to(device)\n",
        "      seg_ids = seg_ids.to(device)\n",
        "      labels = y.to(device)\n",
        "\n",
        "      prediction = model(pair_token_ids, mask_ids, seg_ids)\n",
        "      loss = criteria(prediction, labels)\n",
        "\n",
        "      total_dev_loss += loss.item()\n",
        "\n",
        "  # Calculate the average accuracy and loss over all of the batches.\n",
        "  dev_loss = total_dev_loss/len(dev_loader)\n",
        "  print('dev loss: ', dev_loss)\n",
        "  if dev_loss < best_dev_loss:\n",
        "    best_dev_loss = dev_loss\n",
        "    print('saving checkpoint')\n",
        "    torch.save(model, 'models/bert-mlm.pth')\n",
        "  else:\n",
        "    print('skip saving checkpoint')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "177\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>@stellargirl I loooooooovvvvvveee my Kindle2. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>354</th>\n",
              "      <td>4</td>\n",
              "      <td>After using LaTeX a lot, any other typeset mat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>355</th>\n",
              "      <td>0</td>\n",
              "      <td>On that note, I hate Word. I hate Pages. I hat...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>356</th>\n",
              "      <td>4</td>\n",
              "      <td>Ahhh... back in a *real* text editing environm...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>357</th>\n",
              "      <td>0</td>\n",
              "      <td>Trouble in Iran, I see. Hmm. Iran. Iran so far...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>358</th>\n",
              "      <td>0</td>\n",
              "      <td>Reading the tweets coming out of Iran... The w...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>359 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     label                                           sentence\n",
              "0        4  @stellargirl I loooooooovvvvvveee my Kindle2. ...\n",
              "1        4  Reading my kindle2...  Love it... Lee childs i...\n",
              "2        4  Ok, first assesment of the #kindle2 ...it fuck...\n",
              "3        4  @kenburbary You'll love your Kindle2. I've had...\n",
              "4        4  @mikefish  Fair enough. But i have the Kindle2...\n",
              "..     ...                                                ...\n",
              "354      4  After using LaTeX a lot, any other typeset mat...\n",
              "355      0  On that note, I hate Word. I hate Pages. I hat...\n",
              "356      4  Ahhh... back in a *real* text editing environm...\n",
              "357      0  Trouble in Iran, I see. Hmm. Iran. Iran so far...\n",
              "358      0  Reading the tweets coming out of Iran... The w...\n",
              "\n",
              "[359 rows x 2 columns]"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('data/testdata.manual.2009.06.14.csv', header=None, encoding='latin-1')\n",
        "df = df.loc[:, [0, 5]]\n",
        "df.columns = ['label', 'sentence']\n",
        "print(df[df['label']==0].shape[0])\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>sentence</th>\n",
              "      <th>token</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>@stellargirl I loooooooovvvvvveee my Kindle2. ...</td>\n",
              "      <td>[i, loooooooovvvvvveee, my, kindle2, not, that...</td>\n",
              "      <td>i loooooooovvvvvveee my kindle2 not that the d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
              "      <td>[reading, my, kindle2, love, it, lee, childs, ...</td>\n",
              "      <td>reading my kindle2 love it lee childs is good ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
              "      <td>[ok, first, assesment, of, the, kindle2, it, f...</td>\n",
              "      <td>ok first assesment of the kindle2 it fucking r...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
              "      <td>[youll, love, your, kindle2, ive, had, mine, f...</td>\n",
              "      <td>youll love your kindle2 ive had mine for a few...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
              "      <td>[fair, enough, but, i, have, the, kindle2, and...</td>\n",
              "      <td>fair enough but i have the kindle2 and i think...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   label                                           sentence  \\\n",
              "0      4  @stellargirl I loooooooovvvvvveee my Kindle2. ...   \n",
              "1      4  Reading my kindle2...  Love it... Lee childs i...   \n",
              "2      4  Ok, first assesment of the #kindle2 ...it fuck...   \n",
              "3      4  @kenburbary You'll love your Kindle2. I've had...   \n",
              "4      4  @mikefish  Fair enough. But i have the Kindle2...   \n",
              "\n",
              "                                               token  \\\n",
              "0  [i, loooooooovvvvvveee, my, kindle2, not, that...   \n",
              "1  [reading, my, kindle2, love, it, lee, childs, ...   \n",
              "2  [ok, first, assesment, of, the, kindle2, it, f...   \n",
              "3  [youll, love, your, kindle2, ive, had, mine, f...   \n",
              "4  [fair, enough, but, i, have, the, kindle2, and...   \n",
              "\n",
              "                                                text  \n",
              "0  i loooooooovvvvvveee my kindle2 not that the d...  \n",
              "1  reading my kindle2 love it lee childs is good ...  \n",
              "2  ok first assesment of the kindle2 it fucking r...  \n",
              "3  youll love your kindle2 ive had mine for a few...  \n",
              "4  fair enough but i have the kindle2 and i think...  "
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df['token'] = df['sentence'].apply(sentence_clean)\n",
        "df['text'] = df['token'].apply(lambda t: ' '.join(t))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_text = df.text.values\n",
        "test_label_ = df.label.values\n",
        "\n",
        "for i in range(test_text.shape[0]):\n",
        "  test_text[i] += \" [SEP] this sentence sentiment is [MASK]\"\n",
        "\n",
        "test_labels = []\n",
        "for label in test_label_:\n",
        "  test_labels.append(label_dict[label])\n",
        "\n",
        "teset_input_ids, test_attention_masks, test_token_type_ids, test_labels = processdata(test_text, test_labels)\n",
        "\n",
        "test_dataset = TensorDataset(teset_input_ids, test_attention_masks, test_token_type_ids, test_labels, torch.arange(test_labels.size(0)))\n",
        "\n",
        "test_loader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size = batch_size \n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.8273\n"
          ]
        },
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
          ]
        }
      ],
      "source": [
        "from torchmetrics import Accuracy\n",
        "model = torch.load('models/bert-mlm.pth')\n",
        "model.eval()\n",
        "\n",
        "test_acc = Accuracy(num_classes=2).to(device)\n",
        "with torch.no_grad():\n",
        "    for batch_idx, (pair_token_ids, mask_ids, seg_ids, y, _) in enumerate(test_loader):\n",
        "\n",
        "        #clear any previously calculated gradients before performing a backward pass\n",
        "\n",
        "        pair_token_ids = pair_token_ids.to(device)\n",
        "        mask_ids = mask_ids.to(device)\n",
        "        seg_ids = seg_ids.to(device)\n",
        "        labels = y.to(device)\n",
        "\n",
        "        prediction = model(pair_token_ids, mask_ids, seg_ids).argmax(dim=1)\n",
        "        test_acc(prediction, labels)\n",
        "\n",
        "test_acc = float(test_acc.compute())\n",
        "print('Test accuracy: %.4f' % test_acc)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "pynli",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.15"
    },
    "vscode": {
      "interpreter": {
        "hash": "4a13d02c6839d39988a711bbb1394bfd9339964e0d499da583a7f9a5413b1821"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
